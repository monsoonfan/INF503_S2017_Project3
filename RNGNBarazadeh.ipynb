{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNGNBarazadeh.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYX6PswDXXMU50XLkCZCyl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monsoonfan/INF503_S2017_Project3/blob/master/RNGNBarazadeh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqXj01sTHQhl"
      },
      "source": [
        "## helper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXcxd1DxHUDy"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def folder_creator(folder):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    for the_file in os.listdir(folder):\n",
        "        file_path = os.path.join(folder, the_file)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                os.unlink(file_path)\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "def leaky_relu(x, alpha=0.2):\n",
        "    return tf.maximum(x * alpha, x)\n",
        "\n",
        "def conv2d(x, W):\n",
        "\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "\n",
        "def weight_placeholder(shape, name=None):\n",
        "    return tf.placeholder(tf.float32, shape, name = name)\n",
        "\n",
        "def bias_placeholder(shape, name = None):\n",
        "    return tf.placeholder(tf.float32, shape, name = name)\n",
        "\n",
        "def weight_bias(hyper):\n",
        "\n",
        "    global filter_size, fst_lyr_num_fltrs, scnd_lyr_num_fltrs, third_lyr_num_fltrs, forth_lyr_num_fltrs\n",
        "\n",
        "    filter_size = hyper['filter_size']\n",
        "    fst_lyr_num_fltrs = hyper['fst_lyr_num_fltrs']\n",
        "    scnd_lyr_num_fltrs = hyper['scnd_lyr_num_fltrs']\n",
        "\n",
        "    W_conv1 = weight_placeholder([filter_size, filter_size, 1, fst_lyr_num_fltrs], name = 'W_conv1' )\n",
        "    b_conv1 = bias_placeholder([fst_lyr_num_fltrs], name = 'b_conv1')\n",
        "\n",
        "    W_conv2 = weight_placeholder([filter_size, filter_size, fst_lyr_num_fltrs, scnd_lyr_num_fltrs], name = 'W_conv2')\n",
        "    b_conv2 = bias_placeholder([scnd_lyr_num_fltrs], name = 'b_conv2')\n",
        "\n",
        "    return W_conv1, b_conv1, W_conv2, b_conv2\n",
        "\n",
        "\n",
        "def Generator(z, reuse = False):\n",
        "\n",
        "\n",
        "\n",
        "    with tf.variable_scope(\"Generator\", reuse=reuse):\n",
        "        bs = tf.shape(z)[0]\n",
        "        fc1 = tf.layers.dense(z, 1024)\n",
        "        fc1 = leaky_relu(fc1)\n",
        "        fc2 = tf.layers.dense(fc1, 7 * 7 * 128)\n",
        "        fc2 = tf.reshape(fc2, tf.stack([bs, 7, 7, 128]))\n",
        "        fc2 = leaky_relu(fc2)\n",
        "        conv1 = tf.contrib.layers.conv2d_transpose(fc2, 64, [4, 4], [2, 2])\n",
        "        conv1 = leaky_relu(conv1)\n",
        "        conv2 = tf.contrib.layers.conv2d_transpose(conv1, 1, [4, 4], [2, 2], activation_fn=tf.sigmoid)\n",
        "        conv2 = tf.reshape(conv2, tf.stack([bs, 784]))\n",
        "    return conv2\n",
        "\n",
        "\n",
        "\n",
        "def random_discriminator(u, rr, D1):\n",
        "\n",
        "\n",
        "    u = tf.reshape(u, [-1,28,28,1])\n",
        "    u = leaky_relu(conv2d(u, D1['W_conv1']) + D1['b_conv1'])\n",
        "    u = max_pool_2x2(u)\n",
        "\n",
        "    u = leaky_relu(conv2d(u, D1['W_conv2']) + D1['b_conv2'])\n",
        "    u = max_pool_2x2(u)\n",
        "    u = tf.reshape(u, [-1, 7*7*rr]) #tf.contrib.layers.flatten(u)#\n",
        "\n",
        "\n",
        "    return u\n",
        "\n",
        "\n",
        "def dist(a, b, bs):\n",
        "    c = 0.0 * tf.norm(a[1,] - b[1,])\n",
        "    for j in range(bs):\n",
        "       vec = a[j,]\n",
        "       m = bs\n",
        "       matrix = tf.ones([m, 1]) * vec\n",
        "       temp = tf.norm(matrix - b, axis = 1)\n",
        "\n",
        "       c = c + tf.reduce_min(temp)\n",
        "\n",
        "    return c\n",
        "\n",
        "\n",
        "def data2img(data):\n",
        "    shape = [28, 28, 1]\n",
        "    return np.reshape(data, [data.shape[0]] + shape)\n",
        "\n",
        "def grid_transform2(x, size):\n",
        "    a = 8\n",
        "    b = 16\n",
        "    h, w, c = size[0], size[1], size[2]\n",
        "    x = np.reshape(x, [a, b, h, w, c])\n",
        "    x = np.transpose(x, [0, 2, 1, 3, 4])\n",
        "    x = np.reshape(x, [a * h, b * w, c])\n",
        "    if x.shape[2] == 1:\n",
        "        x = np.squeeze(x, axis=2)\n",
        "    return x\n",
        "\n",
        "def grid_transform(x, size):\n",
        "\n",
        "    a = 10\n",
        "    b = int(x.shape[0]/a)\n",
        "    h, w, c = size[0], size[1], size[2]\n",
        "    x = np.reshape(x, [a, b, h, w, c])\n",
        "    x = np.transpose(x, [0, 2, 1, 3, 4])\n",
        "    x = np.reshape(x, [a * h, b * w, c])\n",
        "    if x.shape[2] == 1:\n",
        "        x = np.squeeze(x, axis=2)\n",
        "    return x\n",
        "\n",
        "def return_normal(shape):\n",
        "    return np.float32(np.random.normal(loc=0.0, scale=0.1, size=shape))\n",
        "\n",
        "def return_const(shape, c = .1):\n",
        "    return np.float32(np.full(shape = shape, fill_value = c))\n",
        "\n",
        "def return_unifrom(shape):\n",
        "    return np.float32(np.random.uniform(-1.0, 1.0, shape))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gnOljF-Cd6h"
      },
      "source": [
        "## main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "s8lvQ5G-CZ3M",
        "outputId": "adc28879-0af1-4449-e42f-eff35f11b16f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import helper\n",
        "#%tensorflow_version 2.x\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import scipy.io as sio\n",
        "import tensorflow as tf\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# from tensorflow.examples.tutorials.mnist import input_data\n",
        "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sample_dir = 'logs/mnist14'\n",
        "sample_dir_2 = './icp_format_results14/'\n",
        "sample_dir_sample = 'logs/sample'\n",
        "\n",
        "z_dim = 100\n",
        "b_size = 100\n",
        "max_iter = 100000\n",
        "step_size = 1e-3\n",
        "iteration = 500\n",
        "hyper_parm_filter = {\n",
        "'filter_size' : 5, ##### Best is 16\n",
        "'fst_lyr_num_fltrs' : 64,\n",
        "'scnd_lyr_num_fltrs' : 64,\n",
        "}\n",
        "rr = hyper_parm_filter['scnd_lyr_num_fltrs']\n",
        "\n",
        "\n",
        "\n",
        "helper.folder_creator(sample_dir)\n",
        "helper.folder_creator(sample_dir_2)\n",
        "helper.folder_creator(sample_dir_sample)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "W_conv1_temp, b_conv1_temp, W_conv2_temp, b_conv2_temp = helper.weight_bias(hyper_parm_filter)\n",
        "W_conv1 = tf.get_variable(dtype = tf.float32, name = 'W_conv1',\n",
        "initializer=W_conv1_temp)\n",
        "b_conv1 = tf.get_variable(dtype = tf.float32,  name = 'b_conv1',\n",
        "initializer=b_conv1_temp)\n",
        "W_conv2 = tf.get_variable(dtype = tf.float32, name = 'W_conv2',\n",
        "initializer=W_conv2_temp)\n",
        "b_conv2 = tf.get_variable(dtype = tf.float32,  name = 'b_conv2',\n",
        "initializer=b_conv2_temp)\n",
        "\n",
        "\n",
        "\n",
        "D1 = {'W_conv1':W_conv1,'b_conv1':b_conv1, 'W_conv2':W_conv2,'b_conv2':b_conv2}\n",
        "init_new_vars_op = tf.initialize_variables([W_conv1, b_conv1, W_conv2, b_conv2])\n",
        "\n",
        "x1 = tf.placeholder(tf.float32, [None, 784], name = 'x1')\n",
        "z1 = tf.placeholder(tf.float32, [None, z_dim], name = 'z1')\n",
        "\n",
        "gz1 = helper.Generator(z1)\n",
        "\n",
        "\n",
        "x1_real_conv = helper.random_discriminator(x1,rr,D1 )\n",
        "\n",
        "\n",
        "gz1_real_conv = helper.random_discriminator(gz1,rr, D1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss_1 = helper.dist(x1_real_conv , gz1_real_conv, b_size)\n",
        "loss_2 = helper.dist( gz1_real_conv, x1_real_conv, b_size)\n",
        "g_loss =  tf.cond(tf.less(loss_1, loss_2), lambda: loss_2, lambda: loss_1)\n",
        "\n",
        "gen_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Generator\")\n",
        "\n",
        "\n",
        "step = tf.Variable(0, trainable=False)\n",
        "rate = tf.train.exponential_decay(step_size, step, 3000, 0.96)\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=rate, beta1=0.5, beta2=0.9, name = 'Adam1')\n",
        "grads_and_vars = optimizer.compute_gradients(g_loss,  var_list=gen_params )\n",
        "vars = [x[1] for x in grads_and_vars]\n",
        "gradients = [x[0] for x in grads_and_vars]\n",
        "gen_train_op = optimizer.apply_gradients(zip(gradients,vars), global_step = step )\n",
        "\n",
        "\n",
        "#\n",
        "def Create_feed_dict():\n",
        "    filter_size = hyper_parm_filter['filter_size']\n",
        "    fst_lyr_num_fltrs = hyper_parm_filter['fst_lyr_num_fltrs']\n",
        "    scnd_lyr_num_fltrs = hyper_parm_filter['scnd_lyr_num_fltrs']\n",
        "    feed = {}\n",
        "    #\n",
        "    feed[b_conv1_temp] = helper.return_const([fst_lyr_num_fltrs], c = 0.0)\n",
        "    #\n",
        "    feed[b_conv2_temp] = helper.return_const([scnd_lyr_num_fltrs], c = 0.0 )\n",
        "    feed[W_conv1_temp] = helper.return_normal([filter_size, filter_size, 1, fst_lyr_num_fltrs])\n",
        "    feed[W_conv2_temp] = helper.return_normal([filter_size, filter_size, fst_lyr_num_fltrs, scnd_lyr_num_fltrs])\n",
        "\n",
        "    return feed\n",
        "\n",
        "def Generate_samples_and_save(fixed_noise, iter = 0, counter = 1):\n",
        "    fake_samples = sess.run(gz1, feed_dict={z1:fixed_noise})\n",
        "\n",
        "    adict = {}\n",
        "    adict['images'] = fake_samples\n",
        "    c = counter\n",
        "    sio.savemat(sample_dir_2+'{}.mat'.format(str(c).zfill(3)), adict)\n",
        "    fake_samples = helper.data2img(fake_samples)\n",
        "    sample_plot = fake_samples[np.random.randint(0, 10000, 128),:]\n",
        "\n",
        "\n",
        "    fake_samples = helper.grid_transform(fake_samples, [28, 28, 1])\n",
        "    fake_samples = np.squeeze(fake_samples)\n",
        "    fake_samples = (255.99*fake_samples).astype('uint8')\n",
        "    plt.imsave(sample_dir+'/samples_'+str(iter)+'.png', fake_samples)\n",
        "    img = Image.open(sample_dir+'/samples_'+str(iter)+'.png').convert('LA')\n",
        "    img.save(sample_dir+'/samples_'+str(iter)+'.png')\n",
        "    sample_plot = helper.grid_transform2(sample_plot, [28, 28, 1])\n",
        "    sample_plot = np.squeeze(sample_plot)\n",
        "    sample_plot = (255.99 * sample_plot).astype('uint8')\n",
        "    plt.imsave(sample_dir_sample + '/samples_' + str(iter) + '.png', sample_plot)\n",
        "    img = Image.open(sample_dir_sample + '/samples_' + str(iter) + '.png').convert('LA')\n",
        "    img.save(sample_dir_sample + '/samples_' + str(iter) + '.png')\n",
        "\n",
        "    return fake_samples\n",
        "\n",
        "\n",
        "\n",
        "fixed_noise = np.random.normal(0.0, 1.0, [10000, z_dim])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    feed2 = Create_feed_dict()\n",
        "    sess.run(tf.global_variables_initializer(),feed_dict = feed2)\n",
        "    iter = 0\n",
        "    Plot_counter = 1\n",
        "    Generate_samples_and_save(fixed_noise, iter = iter, counter = 0 )\n",
        "    while iter < (max_iter):\n",
        "        if iter % 10 == 0:\n",
        "            print(iter)\n",
        "\n",
        "        feed = {}\n",
        "        data1 = mnist.train.next_batch(b_size)[0]\n",
        "        code1 = helper.return_unifrom([b_size , z_dim])\n",
        "        feed[x1] = data1\n",
        "        feed[z1] = code1\n",
        "        _= sess.run(gen_train_op, feed_dict = feed)\n",
        "\n",
        "        iter = iter + 1\n",
        "        feed2 = Create_feed_dict()\n",
        "        sess.run(init_new_vars_op, feed_dict = feed2)\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            Generate_samples_and_save(fixed_noise, iter = iter, counter = Plot_counter )\n",
        "            Plot_counter = Plot_counter  + 1\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4b8711f7ec78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_dir_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_dir_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'helper' has no attribute 'folder_creator'"
          ]
        }
      ]
    }
  ]
}